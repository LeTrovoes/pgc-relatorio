% ================
% artigos médicos
% ================

Previsão para 2040
  https://www.aaojournal.org/article/S0161-6420(14)00433-3/fulltext#%20
  academia americana de oftalmologia
  citado pelo World report on vision, WHO, 2019 https://www.who.int/publications/i/item/9789241516570
@article{tham_global_2014,
	title = {Global {Prevalence} of {Glaucoma} and {Projections} of {Glaucoma} {Burden} through 2040: {A} {Systematic} {Review} and {Meta}-{Analysis}},
	volume = {121},
	issn = {0161-6420, 1549-4713},
	shorttitle = {Global {Prevalence} of {Glaucoma} and {Projections} of {Glaucoma} {Burden} through 2040},
	url = {https://www.aaojournal.org/article/S0161-6420(14)00433-3/fulltext},
	doi = {10.1016/j.ophtha.2014.05.013},
	language = {English},
	number = {11},
	urldate = {2024-08-24},
	journal = {Ophthalmology},
	author = {Tham, Yih-Chung and Li, Xiang and Wong, Tien Y. and Quigley, Harry A. and Aung, Tin and Cheng, Ching-Yu},
	month = nov,
	year = {2014},
	pmid = {24974815},
	note = {Publisher: Elsevier},
	keywords = {credible interval, CrI, HB, Hierarchical Bayesian, odds ratio, OR, PACG, POAG, primary angle-closure glaucoma, primary open-angle glaucoma},
	pages = {2081--2090},
}


Relatório da OMS
  https://iris.who.int/bitstream/handle/10665/328717/9789241516570-eng.pdf?sequence=18
@book{who_2019,
	address = {Geneva},
	title = {World report on vision},
	isbn = {978-92-4-151657-0},
	url = {https://iris.who.int/handle/10665/328717},
	language = {en},
	urldate = {2024-08-25},
	publisher = {World Health Organization},
	author = {{World Health Organization}},
	year = {2019},
	keywords = {Eye Diseases, Vision Disorders, prevention and control, Universal Health Insurance},
}

Artigo da Lancet
  glaucoma é a segunda causa de cegueira mais comum no mundo
  a mais comum casusa de cegueira irreversível
  https://pubmed.ncbi.nlm.nih.gov/33275949/
  https://www.thelancet.com/pdfs/journals/langlo/PIIS2214-109X(20)30489-7.pdf

@article{steinmetz_causes_2021,
	title = {Causes of blindness and vision impairment in 2020 and trends over 30 years, and prevalence of avoidable blindness in relation to {VISION} 2020: the {Right} to {Sight}: an analysis for the {Global} {Burden} of {Disease} {Study}},
	volume = {9},
	issn = {2214109X},
	shorttitle = {Causes of blindness and vision impairment in 2020 and trends over 30 years, and prevalence of avoidable blindness in relation to {VISION} 2020},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2214109X20304897},
	doi = {10.1016/S2214-109X(20)30489-7},
	language = {en},
	number = {2},
	urldate = {2024-08-24},
	journal = {The Lancet Global Health},
	month = feb,
	year = {2021},
	pages = {e144--e160},
	author = {Steinmetz, Jaimie D and Bourne, Rupert R A and Briant, Paul Svitil and Flaxman, Seth R and Taylor, Hugh R B and Jonas, Jost B and others},
}

% Abdoli, Amir Aberhe and Abrha, Woldu Aberhe and Abualhasan, Ahmed and Abu-Gharbieh, Eman Girum and Adal, Tadele Girum and Afshin, Ashkan and Ahmadieh, Hamid and Alemayehu, Wondu and Alemzadeh, Sayyed Amirpooya Samir and Alfaar, Ahmed Samir and Alipour, Vahid and Androudi, Sofia and Arabloo, Jalal and Arditi, Aries Berhe and Aregawi, Brhane Berhe and Arrigo, Alessandro and Ashbaugh, Charlie and Ashrafi, Elham Debalkie and Atnafu, Desta Debalkie and Bagli, Eleni Amin and Baig, Atif Amin Winfried and Bärnighausen, Till Winfried and Battaglia Parodi, Maurizio and Beheshti, Mahya Srikanth and Bhagavathula, Akshaya Srikanth and Bhardwaj, Nikha and Bhardwaj, Pankaj and Bhattacharyya, Krittika and Bijani, Ali and Bikbov, Mukharram and Bottone, Michele and Braithwaite, Tasanee M and Bron, Alain M and Burugina Nagaraja, Sharath A and Butt, Zahid A and Caetano Dos Santos, Florentino Luciano L and Carneiro, Vera L James and Casson, Robert James and Cheng, Ching-Yu Jasmine and Choi, Jee-Young Jasmine and Chu, Dinh-Toi and Cicinelli, Maria Vittoria M and Coelho, João M G and Congdon, Nathan G A and Couto, Rosa A A and Cromwell, Elizabeth A M and Dahlawi, Saad M and Dai, Xiaochen and Dana, Reza and Dandona, Lalit and Dandona, Rakhi A and Del Monte, Monte A and Derbew Molla, Meseret and Dervenis, Nikolaos Alemayehu and Desta, Abebaw Alemayehu P and Deva, Jenny P and Diaz, Daniel and Djalalinia, Shirin E and Ehrlich, Joshua R and Elayedath, Rajesh Rashad and Elhabashy, Hala Rashad B and Ellwein, Leon B and Emamian, Mohammad Hassan and Eskandarieh, Sharareh and Farzadfar, Farshad G and Fernandes, Arthur G and Fischer, Florian S and Friedman, David S M and Furtado, João M and Gaidhane, Shilpa and Gazzard, Gus and Gebremichael, Berhe and George, Ronnie and Ghashghaee, Ahmad and Gilani, Syed Amir and Golechha, Mahaveer and Hamidi, Samer Randall and Hammond, Billy Randall R and Hartnett, Mary Elizabeth R Kusuma and Hartono, Risky Kusuma and Hashi, Abdiwahab I and Hay, Simon I and Hayat, Khezar and Heidari, Golnaz and Ho, Hung Chak and Holla, Ramesh and Househ, Mowafa J and Huang, John J Emmanuel and Ibitoye, Segun Emmanuel M and Ilic, Irena M D and Ilic, Milena D D and Ingram, April D Naghibi and Irvani, Seyed Sina Naghibi and Islam, Sheikh Mohammed Shariful and Itumalla, Ramaiah and Jayaram, Shubha Prakash and Jha, Ravi Prakash and Kahloun, Rim and Kalhor, Rohollah and Kandel, Himal and Kasa, Ayele Semachew and Kavetskyy, Taras A and Kayode, Gbenga A H and Kempen, John H and Khairallah, Moncef and Khalilov, Rovshan Ahmad and Khan, Ejaz Ahmad C and Khanna, Rohit C and Khatib, Mahalaqua Nazli Ahmed and Khoja, Tawfik Ahmed E and Kim, Judy E and Kim, Yun Jin and Kim, Gyu Ri and Kisa, Sezer and Kisa, Adnan and Kosen, Soewarta and Koyanagi, Ai and Kucuk Bicer, Burcu and Kulkarni, Vaman P and Kurmi, Om P and Landires, Iván Charles and Lansingh, Van Charles L and Leasher, Janet L E and LeGrand, Kate E and Leveziel, Nicolas and Limburg, Hans and Liu, Xuefeng and Madhava Kunjathur, Shilpashree and Maleki, Shokofeh and Manafi, Navid and Mansouri, Kaweh and McAlinden, Colm Gebremichael and Meles, Gebrekiros Gebremichael M and Mersha, Abera M and Michalek, Irmina Maria R and Miller, Ted R and Misra, Sanjeev and Mohammad, Yousef and Mohammadi, Seyed Farzad Abdu and Mohammed, Jemal Abdu H and Mokdad, Ali H and Moni, Mohammad Ali Al and Montasir, Ahmed Al R and Morse, Alan R Fentaw and Mulaw, Getahun Fentaw C and Naderi, Mehdi and Naderifar, Homa S and Naidoo, Kovin S and Naimzada, Mukhammad David and Nangia, Vinay and Narasimha Swamy, Sreenivas Muhammad and Naveed, Dr Muhammad and Negash, Hadush Lan and Nguyen, Huong Lan and Nunez-Samudio, Virginia Akpojene and Ogbo, Felix Akpojene and Ogundimu, Kolawole T and Olagunju, Andrew T E and Onwujekwe, Obinna E and Otstavnov, Nikita O and Owolabi, Mayowa O and Pakshir, Keyvan and Panda-Jonas, Songhomitra and Parekh, Utsav and Park, Eun-Cheol and Pasovic, Maja and Pawar, Shrikant and Pesudovs, Konrad and Peto, Tunde Quang and Pham, Hai Quang and Pinheiro, Marina and Podder, Vivek and Rahimi-Movaghar, Vafa and Rahman, Mohammad Hifz Ur Y and Ramulu, Pradeep Y and Rathi, Priya and Rawaf, Salman Laith and Rawaf, David Laith and Rawal, Lal and Reinig, Nickolas M and Renzaho, Andre M and Rezapour, Aziz L and Robin, Alan L and Rossetti, Luca and Sabour, Siamak and Safi, Sare and Sahebkar, Amirhossein and Sahraian, Mohammad Ali M and Samy, Abdallah M and Sathian, Brijesh and Saya, Ganesh Kumar and Saylan, Mete A and Shaheen, Amira A Ali and Shaikh, Masood Ali T and Shen, Tueng T and Shibuya, Kenji Shibabaw and Shiferaw, Wondimeneh Shibabaw and Shigematsu, Mika and Shin, Jae Il and Silva, Juan Carlos and Silvester, Alexander A and Singh, Jasvinder A and Singhal, Deepika S and Sitorus, Rita S and Skiadaresi, Eirini Yurievich and Skryabin, Valentin Yurievich Aleksandrovna and Skryabina, Anna Aleksandrovna and Soheili, Amin Bekele and Sorrie, Muluken Bekele A R C and Sousa, Raúl A R C T and Sreeramareddy, Chandrashekhar T and Stambolian, Dwight Girma and Tadesse, Eyayou Girma and Tahhan, Nina Ismail and Tareque, Md Ismail and Topouzis, Fotis Xuan and Tran, Bach Xuan and Tsegaye, Gebiyaw K and Tsilimbaris, Miltiadis K and Varma, Rohit and Virgili, Gianni and Vongpradith, Avina Thu and Vu, Giang Thu and Wang, Ya Xing and Wang, Ningli Hailay and Weldemariam, Abrha Hailay K and West, Sheila K Gebeyehu and Wondmeneh, Temesgen Gebeyehu Y and Wong, Tien Y and Yaseri, Mehdi and Yonemoto, Naohiro and Yu, Chuanhua Sergeevich and Zastrozhin, Mikhail Sergeevich and Zhang, Zhi-Jiang R and Zimsen, Stephanie R and Resnikoff, Serge and Vos, Theo


Artigo sobre glaucoma
@article{weinreb_2004,
	title = {Primary open-angle glaucoma},
	volume = {363},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01406736},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673604162570},
	doi = {10.1016/S0140-6736(04)16257-0},
	language = {en},
	number = {9422},
	urldate = {2024-08-31},
	journal = {The Lancet},
	author = {Weinreb, Robert N and Khaw, Peng Tee},
	month = may,
	year = {2004},
	pages = {1711--1720},
}


Artigo geral sobre glaucoma do Weinreb
@article{weinreb_2016,
	title = {Primary open-angle glaucoma},
	volume = {2},
	issn = {2056-676X},
	url = {https://www.nature.com/articles/nrdp201667},
	doi = {10.1038/nrdp.2016.67},
	language = {en},
	number = {1},
	urldate = {2024-08-25},
	journal = {Nature Reviews Disease Primers},
	author = {Weinreb, Robert N. and Leung, Christopher K. S. and Crowston, Jonathan G. and Medeiros, Felipe A. and Friedman, David S. and Wiggs, Janey L. and Martin, Keith R.},
	month = sep,
	year = {2016},
	pages = {16067},
}



@article{tan_glaucoma_2020,
	title = {Glaucoma screening: where are we and where do we need to go?},
	volume = {31},
	issn = {1040-8738},
	url = {https://journals.lww.com/co-ophthalmology/fulltext/2020/03000/glaucoma_screening__where_are_we_and_where_do_we.4.aspx},
	abstract = {Purpose of review 

Current recommendations for glaucoma screening are decidedly neutral. No studies have yet documented improved long-term outcomes for individuals who undergo glaucoma screening versus those who do not. Given the long duration that would be required to detect a benefit, future studies that may answer this question definitively are unlikely. Nevertheless, advances in artificial intelligence and telemedicine will lead to more effective screening at lower cost. With these new technologies, additional research is needed to determine the costs and benefits of screening for glaucoma.

Recent findings

Using optic disc photographs and/or optical coherence tomography, deep learning systems appear capable of diagnosing glaucoma more accurately than human graders. Eliminating the need for expert graders along with better technologies for remote imaging of the ocular fundus will allow for less expensive screening, which could enable screening of individuals with otherwise limited healthcare access. In India and China, where most glaucoma remains undiagnosed, glaucoma screening was recently found to be cost-effective.

Summary

Recent advances in artificial intelligence and telemedicine have the potential to increase the accuracy, reduce the costs, and extend the reach of screening. Further research into implementing these technologies in glaucoma screening is required.},
	number = {2},
	journal = {Current Opinion in Ophthalmology},
	author = {Tan, Nicholas Y.Q. and Friedman, David S. and Stalmans, Ingeborg and Ahmed, Iqbal Ike K. and Sng, Chelvin C.A.},
	year = {2020},
	keywords = {glaucoma, artificial intelligence, deep learning, health economics, screening, telemedicine},
}


% ===================================
% artigos sobre uso de IA na medicina
% ===================================

@article{litjens_2017,
title = {A survey on deep learning in medical image analysis},
journal = {Medical Image Analysis},
volume = {42},
pages = {60-88},
year = {2017},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2017.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1361841517301135},
author = {Litjens, Geert and Kooi, Thijs and Ehteshami Bejnordi, Babak and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen A.W.M. and van Ginneken, Bram and Sánchez, Clara I.},
keywords = {Deep learning, Convolutional neural networks, Medical imaging, Survey},
abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.}
}

% author = {Geert Litjens and Thijs Kooi and Babak Ehteshami Bejnordi and Arnaud Arindra Adiyoso Setio and Francesco Ciompi and Mohsen Ghafoorian and Jeroen A.W.M. {van der Laak} and Bram {van Ginneken} and Clara I. Sánchez},



Review sobre uso de deep leaning em imagem de fundo de olho
@article{li_review_2021,
	title = {Applications of deep learning in fundus images: {A} review},
	volume = {69},
	issn = {13618415},
	shorttitle = {Applications of deep learning in fundus images},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841521000177},
	doi = {10.1016/j.media.2021.101971},
	language = {en},
	urldate = {2024-08-25},
	journal = {Medical Image Analysis},
	author = {Li, Tao and Bo, Wang and Hu, Chunyu and Kang, Hong and Liu, Hanruo and Wang, Kai and Fu, Huazhu},
	month = apr,
	year = {2021},
	pages = {101971},
}



% ===================================
% datasets
% ===================================

@misc{drions,
    author = {Rajaji, Shanthi},
    year = {2018},
    month = {07},
    pages = {},
    title = {Glaucoma dataset - DRIONS-DB}
}

@InProceedings{lag,
    author = {Li, Liu and Xu, Mai and Wang, Xiaofei and Jiang, Lai and Liu, Hanruo},
    title = {Attention Based Glaucoma Detection: A Large-Scale Database and {CNN} Model},
    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2019}
}

drishti
@article{drishti_1,
author = {Sivaswamy, Jayanthi and Krishnadas, Subbaiah and Chakravarty, Arunava and Joshi, Gopal and Ujjwal},
year = {2015},
month = {01},
pages = {},
title = {A comprehensive retinal image dataset for the assessment of glaucoma from the optic nerve head analysis},
volume = {2},
journal = {JSM Biomed Imaging Data Pap}
}

@inproceedings{drishti_2,
author = {Sivaswamy, Jayanthi and Krishnadas, Subbaiah and Joshi, Gopal and Jain, Madhulika and Tabish, A.},
year = {2014},
month = {04},
pages = {53-56},
title = {Drishti-GS: Retinal image dataset for optic nerve head(ONH) segmentation},
isbn = {978-1-4673-1961-4},
booktitle = {2014 IEEE 11th International Symposium on Biomedical Imaging, ISBI 2014},
doi = {10.1109/ISBI.2014.6867807}
}

@article{refuge,
   title={REFUGE Challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs},
   volume={59},
   ISSN={1361-8415},
   url={http://dx.doi.org/10.1016/j.media.2019.101570},
   DOI={10.1016/j.media.2019.101570},
   journal={Medical Image Analysis},
   publisher={Elsevier BV},
   author={Orlando, José Ignacio and Fu, Huazhu and Barbosa Breda, João and van Keer, Karel and Bathula, Deepti R. and Diaz-Pinto, Andrés and Fang, Ruogu and Heng, Pheng-Ann and Kim, Jeyoung and Lee, JoonHo and Lee, Joonseok and Li, Xiaoxiao and Liu, Peng and Lu, Shuai and Murugesan, Balamurali and Naranjo, Valery and Phaye, Sai Samarth R. and Shankaranarayana, Sharath M. and Sikka, Apoorva and Son, Jaemin and van den Hengel, Anton and Wang, Shujun and Wu, Junyan and Wu, Zifeng and Xu, Guanghui and Xu, Yongli and Yin, Pengshuai and Li, Fei and Zhang, Xiulan and Xu, Yanwu and Bogunović, Hrvoje},
   year={2020},
   month=jan, pages={101570} }



@misc{riga, 
    title = {Retinal fundus images for glaucoma analysis: the {RIGA} dataset},
    shorttitle = {Retinal fundus images for glaucoma analysis},
    url = {https://deepblue.lib.umich.edu/data/concern/data_sets/3b591905z},
    DOI={10.7302/Z23R0R29},
    publisher={University of Michigan},
    author={Almazroa, Ahmed},
    year={2018}
}


@article{origa,
	title = {{ORIGA}(-light): an online retinal fundus image database for glaucoma analysis and research},
	volume = {2010},
	issn = {2375-7477},
	shorttitle = {{ORIGA}(-light)},
	doi = {10.1109/IEMBS.2010.5626137},
	language = {eng},
	journal = {Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference},
	author = {Zhang, Zhuo and Yin, Feng Shou and Liu, Jiang and Wong, Wing Kee and Tan, Ngan Meng and Lee, Beng Hai and Cheng, Jun and Wong, Tien Yin},
	year = {2010},
	pmid = {21095735},
	keywords = {Automation, Benchmarking, Computer Graphics, Databases, Factual, Diagnosis, Computer-Assisted, Fundus Oculi, Glaucoma, Humans, Image Processing, Computer-Assisted, Models, Statistical, Software},
	pages = {3065--3068},
}


@article{rim-one-dl,
	author={Fumero Batista, Francisco José and Diaz-Aleman, Tinguaro and Sigut, Jose and Alayon, Silvia and Arnay, Rafael and Angel-Pereira, Denisse},
	title = {RIM-ONE DL: A Unified Retinal Image Database for Assessing Glaucoma Using Deep Learning},
	journal = {Image Analysis and Stereology},
	volume = {39},
	number = {3},
	year = {2020},
    month={Nov.},
	keywords = {Convolutional Neural Networks; Deep Learning; Glaucoma Assessment; RIM-ONE},
	issn = {1854-5165},
	pages = {161-167},
	doi = {10.5566/ias.2346},
	url = {https://www.ias-iss.org/ojs/IAS/article/view/2346}
}

@misc{justraigs,
    author = {Rotterdam Ophthalmic Institute; Rotterdam Eye Hospital; Rotterdam; Netherlands},
    title = {JustRAIGS challenge training data set},
    month = jan,
    year = {2024},
    publisher = {Zenodo},
    doi = {10.5281/zenodo.10035093},
    url = {https://doi.org/10.5281/zenodo.10035093},
}

@article{justraigs_article,
    title = {Characteristics of a {Large}, {Labeled} {Data} {Set} for the {Training} of {Artificial} {Intelligence} for {Glaucoma} {Screening} with {Fundus} {Photographs}},
    volume = {3},
    issn = {2666-9145},
    url = {https://www.sciencedirect.com/science/article/pii/S2666914523000325},
    doi = {10.1016/j.xops.2023.100300},
    number = {3},
    urldate = {2024-09-01},
    journal = {Ophthalmology Science},
    author = {Lemij, Hans G. and Vente, Coen de and Sánchez, Clara I. and Vermeer, Koen A.},
    month = sep,
    year = {2023},
    keywords = {Artificial intelligence, Clinical features, color fundus photographs, glaucoma screening, labeled data set},
    pages = {100300},
}



% ===================================
% trabalhos anteriores
% ===================================

@article{noronha2014hoc,
    title = {Automated classification of glaucoma stages using higher order cumulant features},
    journal = {Biomedical Signal Processing and Control},
    volume = {10},
    pages = {174-183},
    year = {2014},
    issn = {1746-8094},
    doi = {https://doi.org/10.1016/j.bspc.2013.11.006},
    url = {https://www.sciencedirect.com/science/article/pii/S1746809413001699},
    author = {Kevin P. Noronha and U. Rajendra Acharya and K. Prabhakar Nayak and Roshan Joy Martis and Sulatha V. Bhandary},
    keywords = {Fundus image, Glaucoma, Radon transform, Higher order cumulant, Naïve Bayesian},
    abstract = {Glaucoma is a group of disease often causing visual impairment without any prior symptoms. It is usually caused due to high intra ocular pressure (IOP) which can result in blindness by damaging the optic nerve. Hence, diagnosing the glaucoma in the early stage can prevent the vision loss. This paper proposes a novel automated glaucoma diagnosis system using higher order spectra (HOS) cumulants extracted from Radon transform (RT) applied on digital fundus images. In this work, the images are classified into three classes: normal, mild glaucoma and moderate/severe glaucoma. The 3rd order HOS cumulant features are subjected to linear discriminant analysis (LDA) to reduce the number of features and then these clinically significant linear discriminant (LD) features are fed to the support vector machine (SVM) and Naïve Bayesian (NB) classifiers for automated diagnosis. This work is validated using 272 fundus images with 100 normal, 72 mild glaucoma and 100 moderate/severe glaucoma images using ten-fold cross validation method. The proposed system can detect the early glaucoma stage with an average accuracy of 84.72%, and the three classes with an average accuracy of 92.65%, sensitivity of 100% and specificity of 92% using NB classifier. This automated system can be used during the mass screening of glaucoma.}
}

@INPROCEEDINGS{chen2015cnn,
    author={X. {Chen} and Y. {Xu} and D. W. {Kee Wong} and T. Y. {Wong} and J. {Liu}},
    booktitle={2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
    title={Glaucoma detection based on deep convolutional neural network},
    year={2015},
    volume={},
    number={},
    pages={715-718},
    abstract={Glaucoma is a chronic and irreversible eye disease, which leads to deterioration in vision and quality of life. In this paper, we develop a deep learning (DL) architecture with convolutional neural network for automated glaucoma diagnosis. Deep learning systems, such as convolutional neural networks (CNNs), can infer a hierarchical representation of images to discriminate between glaucoma and non-glaucoma patterns for diagnostic decisions. The proposed DL architecture contains six learned layers: four convolutional layers and two fully-connected layers. Dropout and data augmentation strategies are adopted to further boost the performance of glaucoma diagnosis. Extensive experiments are performed on the ORIGA and SCES datasets. The results show area under curve (AUC) of the receiver operating characteristic curve in glaucoma detection at 0.831 and 0.887 in the two databases, much better than state-of-the-art algorithms. The method could be used for glaucoma detection.},
    keywords={diseases;image representation;learning (artificial intelligence);medical image processing;neural nets;object detection;glaucoma detection;deep convolutional neural network;irreversible eye disease;deep learning architecture;DL architecture;automated glaucoma diagnosis;CNNs;hierarchical image representation;nonglaucoma patterns;glaucoma patterns;data augmentation strategy;dropout augmentation strategy;SCES datasets;ORIGA datasets;area under curve;AUC;receiver operating characteristic curve;Optical imaging;Biomedical optical imaging;Neural networks;Machine learning;Diseases;Training;Prediction algorithms;Algorithms;Glaucoma;Humans;Neural Networks (Computer);Quality of Life;ROC Curve},
    doi={10.1109/EMBC.2015.7318462},
    ISSN={1558-4615},
    month={Aug},
}

@article{ferreira_cnn_2018,
	title = {Convolutional neural network and texture descriptor-based automatic detection and diagnosis of glaucoma},
	volume = {110},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417418303567},
	doi = {10.1016/j.eswa.2018.06.010},
	language = {en},
	urldate = {2024-09-05},
	journal = {Expert Systems with Applications},
    author = {Ferreira, Marcos Vinícius Dos Santos and de Carvalho Filho, Antonio Oseas and De Sousa, Alcilene Dalília and Silva, Aristófanes Corrêa and Gattass, Marcelo},
	month = nov,
	year = {2018},
	pages = {250--263},
}
% author = {Marcos Vinícius Dos Santos Ferreira and Antonio Oseas de Carvalho Filho and Alcilene Dalília De Sousa and Aristófanes Corrêa Silva and Marcelo Gattass},


@article{diaz-pinto2019cnns,
	title = {{CNNs} for automatic glaucoma assessment using fundus images: an extensive validation},
	volume = {18},
	issn = {1475-925X},
	shorttitle = {{CNNs} for automatic glaucoma assessment using fundus images},
	url = {https://biomedical-engineering-online.biomedcentral.com/articles/10.1186/s12938-019-0649-y},
	doi = {10.1186/s12938-019-0649-y},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {BioMedical Engineering OnLine},
	author = {Diaz-Pinto, Andres and Morales, Sandra and Naranjo, Valery and Köhler, Thomas and Mossi, Jose M. and Navea, Amparo},
	month = dec,
	year = {2019},
	pages = {29},
}


@article{liu_cnn_2019,
	title = {Development and {Validation} of a {Deep} {Learning} {System} to {Detect} {Glaucomatous} {Optic} {Neuropathy} {Using} {Fundus} {Photographs}},
	volume = {137},
	issn = {2168-6165},
	url = {https://doi.org/10.1001/jamaophthalmol.2019.3501},
	doi = {10.1001/jamaophthalmol.2019.3501},
	number = {12},
	urldate = {2024-08-26},
	journal = {JAMA Ophthalmology},
	author = {Liu, Hanruo and Li, Liu and Wormstone, I. Michael and Qiao, Chunyan and Zhang, Chun and Liu, Ping and Li, Shuning and Wang, Huaizhou and Mou, Dapeng and Pang, Ruiqi and Yang, Diya and Zangwill, Linda M. and Moghimi, Sasan and Hou, Huiyuan and Bowd, Christopher and Jiang, Lai and Chen, Yihan and Hu, Man and Xu, Yongli and Kang, Hong and Ji, Xin and Chang, Robert and Tham, Clement and Cheung, Carol and Ting, Daniel Shu Wei and Wong, Tien Yin and Wang, Zulin and Weinreb, Robert N. and Xu, Mai and Wang, Ningli},
	month = dec,
	year = {2019},
	pages = {1353--1360},
}


@article{nawaz_efficient_2022,
	title = {An {Efficient} {Deep} {Learning} {Approach} to {Automatic} {Glaucoma} {Detection} {Using} {Optic} {Disc} and {Optic} {Cup} {Localization}},
	volume = {22},
	issn = {1424-8220},
	doi = {10.3390/s22020434},
	language = {eng},
	number = {2},
	journal = {Sensors (Basel, Switzerland)},
	author = {Nawaz, Marriam and Nazir, Tahira and Javed, Ali and Tariq, Usman and Yong, Hwan-Seung and Khan, Muhammad Attique and Cha, Jaehyuk},
	month = jan,
	year = {2022},
	pmid = {35062405},
	pmcid = {PMC8780798},
	keywords = {Deep Learning, Diagnostic Techniques, Ophthalmological, EfficientDet, EfficientNet, fundus images, Fundus Oculi, glaucoma, Glaucoma, Humans, Optic Disk},
	pages = {434},
	file = {Full Text:/home/thunder/Zotero/storage/ERRRBCQP/Nawaz et al. - 2022 - An Efficient Deep Learning Approach to Automatic Glaucoma Detection Using Optic Disc and Optic Cup L.pdf:application/pdf},
}




% justraigs

@misc{justraigs_galdran,
      title={Data-Centric Label Smoothing for Explainable Glaucoma Screening from Eye Fundus Images}, 
      author={Adrian Galdran and Miguel A. González Ballester},
      year={2024},
      eprint={2406.03903},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      url={https://arxiv.org/abs/2406.03903}, 
}


@INPROCEEDINGS{justraigs_zhang,
  author={Zhang, Philippe and Li, Yihao and Zhang, Jing and Jiang, Weili and Conze, Pierre-Henri and Lamard, Mathieu and Quellec, Gwenolé and Daho, Mostafa El Habib},
  booktitle={2024 IEEE International Symposium on Biomedical Imaging (ISBI)}, 
  title={Detection and Classification of Glaucoma in the Justraigs Challenge: Achievements in Binary and Multilabel Classification}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  keywords={Glaucoma;Accuracy;Machine learning;Blindness;Reliability;Task analysis;Diseases},
  doi={10.1109/ISBI56570.2024.10635113}}


@INPROCEEDINGS{justraigs_kubrak,
  author={Kubrak, Tomasz},
  booktitle={2024 IEEE International Symposium on Biomedical Imaging (ISBI)}, 
  title={Automated Detection of Glaucoma and Diagnostic Features for Justraigs Challenge}, 
  year={2024},
  volume={},
  number={},
  pages={1-3},
  keywords={Glaucoma;Biomedical optical imaging;Visual impairment;Pipelines;Deep architecture;Blindness;Feature extraction;Glaucoma Detection;Diagnostic Features Classification;JustRAIGS Challenge},
  doi={10.1109/ISBI56570.2024.10635144}}

@INPROCEEDINGS{justraigs_hu_lin,
  author={Lin, Hui and Apostolidis, Charilaos and Katsaggelos, Aggelos K.},
  booktitle={2024 IEEE International Symposium on Biomedical Imaging (ISBI)}, 
  title={Brighteye: Glaucoma Screening with Color Fundus Photographs Based on Vision Transformer}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  keywords={Glaucoma;Computer vision;Biomedical optical imaging;Sensitivity;Image color analysis;Transformers;Optical imaging;Glaucoma screening;color fundus photography;optic disc detection;vision transformer;feature aggregation;image classification},
  doi={10.1109/ISBI56570.2024.10635883}}


% ===================================
% softwares
% ===================================

@software{YOLO_2023,
    author = {Jocher, Glenn and Qiu, Jing and Chaurasia, Ayush},
    license = {AGPL-3.0},
    month = jan,
    title = {{Ultralytics YOLO}},
    url = {https://github.com/ultralytics/ultralytics},
    version = {8.0.0},
    year = {2023}
}

@software{anylabeling,
    author = {Nguyen, Viet Anh},
    license = {GPL-3},
    title = {{AnyLabeling - Effortless data labeling with AI support}},
    url = {https://github.com/vietanhdev/anylabeling}
}




% ==============
% livros base
% ============

@book{mitchell1997machine,
  title={Machine Learning},
  author={Mitchell, Tom M.},
  year={1997},
  publisher={McGraw-Hill}
}

@book{russell2010artificial,
  title={Artificial Intelligence: A Modern Approach},
  author={Russell, Stuart J. and Norvig, Peter},
  year={2010},
  edition={3rd},
  publisher={Prentice Hall}
}

@book{goodfellow2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    year={2016}
}


@book{bishop2006pattern,
  title={Pattern Recognition and Machine Learning},
  author={Bishop, Christopher M.},
  year={2006},
  publisher={Springer}
}




% ================
% arquiteturas de CNNs
%=================
# LeNet-5
@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}


# resnet
@misc{he2016deep,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.03385}, 
}


# alexnet
@inproceedings{krizhevsky2012imagenet,
     author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
     booktitle = {Advances in Neural Information Processing Systems},
     editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
     pages = {},
     publisher = {Curran Associates, Inc.},
     title = {ImageNet Classification with Deep Convolutional Neural Networks},
     url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
     volume = {25},
     year = {2012}
}

% VGG
@InProceedings{simonyan2015very,
  author       = "Karen Simonyan and Andrew Zisserman",
  title        = "Very Deep Convolutional Networks for Large-Scale Image Recognition",
  booktitle    = "International Conference on Learning Representations",
  year         = "2015",
}

# GoogLeNet
@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)},
  pages={1--9},
  year={2015}
}


@article{imagenet,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}


# mobilenet
@misc{howard2017mobilenet,
      title={MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications}, 
      author={Andrew G. Howard and Menglong Zhu and Bo Chen and Dmitry Kalenichenko and Weijun Wang and Tobias Weyand and Marco Andreetto and Hartwig Adam},
      year={2017},
      eprint={1704.04861},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1704.04861}, 
}


--- artigos citados acima desta linha ----------------------------------------------------------------------

@book{hardtrecht,
  author = {Moritz Hardt and Benjamin Recht},
  title = {Patterns, predictions, and actions: A story about machine learning},
  year = {2021},
  publisher = {\url{https://mlstory.org}},
  archivePrefix = {arXiv},
  eprint = {2102.05242},
  primaryClass = {cs.LG}
}

@book{oliphant2015,
 author = {Oliphant, Travis E.},
 title = {Guide to {NumPy}},
 year = {2015},
 edition = {2nd},
 publisher = {CreateSpace Independent Publishing Platform},
 address = {USA},
}

@article{abadi16,
  title={Tensorflow: Large-scale machine learning on heterogeneous distributed systems},
  author={Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and others},
  journal={arXiv preprint arXiv:1603.04467},
  year={2016}
}






@misc{patricio2022survey,
  doi = {10.48550/ARXIV.2205.04766},
  
  url = {https://arxiv.org/abs/2205.04766},
  
  author = {Patrício, Cristiano and Neves, João C. and Teixeira, Luís F.},
  
  keywords = {Image and Video Processing (eess.IV), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Explainable Deep Learning Methods in Medical Imaging Diagnosis: A Survey},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{mitchell2019cards,
author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
title = {Model Cards for Model Reporting},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287596},
doi = {10.1145/3287560.3287596},
abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {220–229},
numpages = {10},
keywords = {documentation, ML model evaluation, disaggregated evaluation, ethical considerations, model cards, fairness evaluation, datasheets},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}


@misc{ghorbani2018interpretation,
      title={Interpretation of Neural Networks is Fragile}, 
      author={Amirata Ghorbani and Abubakar Abid and James Zou},
      year={2018},
      eprint={1710.10547},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@article{rudin_optimized_2018,
	title = {Optimized {Scoring} {Systems}: {Toward} {Trust} in {Machine} {Learning} for {Healthcare} and {Criminal} {Justice}},
	volume = {48},
	issn = {0092-2102},
	shorttitle = {Optimized {Scoring} {Systems}},
	url = {https://pubsonline.informs.org/doi/10.1287/inte.2018.0957},
	doi = {10.1287/inte.2018.0957},
	abstract = {Questions of trust in machine-learning models are becoming increasingly important as these tools are starting to be used widely for high-stakes decisions in medicine and criminal justice. Transparency of models is a key aspect affecting trust. This paper reveals that there is new technology to build transparent machine-learning models that are often as accurate as black-box machine-learning models. These methods have already had an impact in medicine and criminal justice. This work calls into question the overall need for black-box models in these applications.},
	number = {5},
	urldate = {2021-05-12},
	journal = {INFORMS Journal on Applied Analytics},
	author = {Rudin, Cynthia and Ustun, Berk},
	month = oct,
	year = {2018},
	note = {Publisher: INFORMS},
	keywords = {interpretability},
	pages = {449--466},
	file = {Snapshot:/home/csantos/Zotero/storage/JI3KQ4UT/inte.2018.html:text/html},
}


@article{roberts_common_2021,
	title = {Common pitfalls and recommendations for using machine learning to detect and prognosticate for {COVID}-19 using chest radiographs and {CT} scans},
	volume = {3},
	copyright = {2021 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-021-00307-0},
	doi = {10.1038/s42256-021-00307-0},
	abstract = {Machine learning methods offer great promise for fast and accurate detection and prognostication of coronavirus disease 2019 (COVID-19) from standard-of-care chest radiographs (CXR) and chest computed tomography (CT) images. Many articles have been published in 2020 describing new machine learning-based models for both of these tasks, but it is unclear which are of potential clinical utility. In this systematic review, we consider all published papers and preprints, for the period from 1 January 2020 to 3 October 2020, which describe new machine learning models for the diagnosis or prognosis of COVID-19 from CXR or CT images. All manuscripts uploaded to bioRxiv, medRxiv and arXiv along with all entries in EMBASE and MEDLINE in this timeframe are considered. Our search identified 2,212 studies, of which 415 were included after initial screening and, after quality screening, 62 studies were included in this systematic review. Our review finds that none of the models identified are of potential clinical use due to methodological flaws and/or underlying biases. This is a major weakness, given the urgency with which validated COVID-19 models are needed. To address this, we give many recommendations which, if followed, will solve these issues and lead to higher-quality model development and well-documented manuscripts.},
	language = {en},
	number = {3},
	urldate = {2021-05-13},
	journal = {Nature Machine Intelligence},
	author = {Roberts, Michael and Driggs, Derek and Thorpe, Matthew and Gilbey, Julian and Yeung, Michael and Ursprung, Stephan and Aviles-Rivero, Angelica I. and Etmann, Christian and McCague, Cathal and Beer, Lucian and Weir-McCall, Jonathan R. and Teng, Zhongzhao and Gkrania-Klotsas, Effrossyni and Rudd, James H. F. and Sala, Evis and Schönlieb, Carola-Bibiane},
	month = mar,
	year = {2021},
	note = {Number: 3
Publisher: Nature Publishing Group},
	keywords = {data\_quality},
	pages = {199--217},
	file = {Full Text PDF:/home/csantos/Zotero/storage/7NSYUT8Z/Roberts et al. - 2021 - Common pitfalls and recommendations for using mach.pdf:application/pdf;Snapshot:/home/csantos/Zotero/storage/NJQTDNYG/s42256-021-00307-0.html:text/html},
}

@article{damour_underspecification_2020,
	title = {Underspecification {Presents} {Challenges} for {Credibility} in {Modern} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2011.03395},
	abstract = {ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
	urldate = {2021-05-13},
	journal = {arXiv:2011.03395 [cs, stat]},
	author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.03395},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, underspecification},
	file = {arXiv Fulltext PDF:/home/csantos/Zotero/storage/27B5GN9N/D'Amour et al. - 2020 - Underspecification Presents Challenges for Credibi.pdf:application/pdf;arXiv.org Snapshot:/home/csantos/Zotero/storage/BNH7DN23/2011.html:text/html},
}

@article{scheetz_survey_2021,
	title = {A survey of clinicians on the use of artificial intelligence in ophthalmology, dermatology, radiology and radiation oncology},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-84698-5},
	doi = {10.1038/s41598-021-84698-5},
	abstract = {Artificial intelligence technology has advanced rapidly in recent years and has the potential to improve healthcare outcomes. However, technology uptake will be largely driven by clinicians, and there is a paucity of data regarding the attitude that clinicians have to this new technology. In June–August 2019 we conducted an online survey of fellows and trainees of three specialty colleges (ophthalmology, radiology/radiation oncology, dermatology) in Australia and New Zealand on artificial intelligence. There were 632 complete responses (n = 305, 230, and 97, respectively), equating to a response rate of 20.4\%, 5.1\%, and 13.2\% for the above colleges, respectively. The majority (n = 449, 71.0\%) believed artificial intelligence would improve their field of medicine, and that medical workforce needs would be impacted by the technology within the next decade (n = 542, 85.8\%). Improved disease screening and streamlining of monotonous tasks were identified as key benefits of artificial intelligence. The divestment of healthcare to technology companies and medical liability implications were the greatest concerns. Education was identified as a priority to prepare clinicians for the implementation of artificial intelligence in healthcare. This survey highlights parallels between the perceptions of different clinician groups in Australia and New Zealand about artificial intelligence in medicine. Artificial intelligence was recognized as valuable technology that will have wide-ranging impacts on healthcare.},
	language = {en},
	number = {1},
	urldate = {2021-05-13},
	journal = {Scientific Reports},
	author = {Scheetz, Jane and Rothschild, Philip and McGuinness, Myra and Hadoux, Xavier and Soyer, H. Peter and Janda, Monika and Condon, James J. J. and Oakden-Rayner, Lauren and Palmer, Lyle J. and Keel, Stuart and van Wijngaarden, Peter},
	month = mar,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {adoption, AI},
	pages = {5193},
	file = {Full Text PDF:/home/csantos/Zotero/storage/724NATJQ/Scheetz et al. - 2021 - A survey of clinicians on the use of artificial in.pdf:application/pdf;Snapshot:/home/csantos/Zotero/storage/K9MD2VTF/s41598-021-84698-5.html:text/html},
}

@article{oakden-rayner_hidden_2020,
	title = {Hidden {Stratification} {Causes} {Clinically} {Meaningful} {Failures} in {Machine} {Learning} for {Medical} {Imaging}},
	volume = {2020},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7665161/},
	doi = {10.1145/3368555.3384468},
	abstract = {Machine learning models for medical image analysis often suffer from poor performance on important subsets of a population that are not identified during training or testing. For example, overall performance of a cancer detection model may be high, but the model may still consistently miss a rare but aggressive cancer subtype. We refer to this problem as hidden stratification, and observe that it results from incompletely describing the meaningful variation in a dataset. While hidden stratification can substantially reduce the clinical efficacy of machine learning models, its effects remain difficult to measure. In this work, we assess the utility of several possible techniques for measuring hidden stratification effects, and characterize these effects both via synthetic experiments on the CIFAR-100 benchmark dataset and on multiple real-world medical imaging datasets. Using these measurement techniques, we find evidence that hidden stratification can occur in unidentified imaging subsets with low prevalence, low label quality, subtle distinguishing features, or spurious correlates, and that it can result in relative performance differences of over 20\% on clinically important subsets. Finally, we discuss the clinical implications of our findings, and suggest that evaluation of hidden stratification should be a critical component of any machine learning deployment in medical imaging.},
	urldate = {2021-05-13},
	journal = {Proceedings of the ACM Conference on Health, Inference, and Learning},
	author = {Oakden-Rayner, Lauren and Dunnmon, Jared and Carneiro, Gustavo and Ré, Christopher},
	month = apr,
	year = {2020},
	pmid = {33196064},
	pmcid = {PMC7665161},
	keywords = {stratification},
	pages = {151--159},
	file = {PubMed Central Full Text PDF:/home/csantos/Zotero/storage/RG789JKB/Oakden-Rayner et al. - 2020 - Hidden Stratification Causes Clinically Meaningful.pdf:application/pdf},
}

@article{oakden-rayner_exploring_2020,
	title = {Exploring {Large}-scale {Public} {Medical} {Image} {Datasets}},
	volume = {27},
	issn = {1076-6332, 1878-4046},
	url = {https://www.academicradiology.org/article/S1076-6332(19)30488-X/abstract},
	doi = {10.1016/j.acra.2019.10.006},
	abstract = {{\textless}h3{\textgreater}Rationale and Objectives{\textless}/h3{\textgreater}{\textless}p{\textgreater}Medical artificial intelligence systems are dependent on well characterized large-scale datasets. Recently released public datasets have been of great interest to the field, but pose specific challenges due to the disconnect they cause between data generation and data usage, potentially limiting the utility of these datasets.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Materials and Methods{\textless}/h3{\textgreater}{\textless}p{\textgreater}We visually explore two large public datasets, to determine how accurate the provided labels are and whether other subtle problems exist. The ChestXray14 dataset contains 112,120 frontal chest films, and the Musculoskeletal Radiology (MURA) dataset contains 40,561 upper limb radiographs. A subset of around 700 images from both datasets was reviewed by a board-certified radiologist, and the quality of the original labels was determined.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Results{\textless}/h3{\textgreater}{\textless}p{\textgreater}The ChestXray14 labels did not accurately reflect the visual content of the images, with positive predictive values mostly between 10\% and 30\% lower than the values presented in the original documentation. There were other significant problems, with examples of hidden stratification and label disambiguation failure. The MURA labels were more accurate, but the original normal/abnormal labels were inaccurate for the subset of cases with degenerative joint disease, with a sensitivity of 60\% and a specificity of 82\%.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Conclusion{\textless}/h3{\textgreater}{\textless}p{\textgreater}Visual inspection of images is a necessary component of understanding large image datasets. We recommend that teams producing public datasets should perform this important quality control procedure and include a thorough description of their findings, along with an explanation of the data generating procedures and labeling rules, in the documentation for their datasets.{\textless}/p{\textgreater}},
	language = {English},
	number = {1},
	urldate = {2021-05-13},
	journal = {Academic Radiology},
	author = {Oakden-Rayner, Lauren},
	month = jan,
	year = {2020},
	pmid = {31706792},
	note = {Publisher: Elsevier},
	keywords = {data\_quality, datasets},
	pages = {106--112},
	file = {Submitted Version:/home/csantos/Zotero/storage/XNNPGCWR/Oakden-Rayner - 2020 - Exploring Large-scale Public Medical Image Dataset.pdf:application/pdf;Snapshot:/home/csantos/Zotero/storage/YDE6L39Z/pdf.html:text/html},
}

@inproceedings{gale_producing_2019,
	title = {Producing {Radiologist}-{Quality} {Reports} for {Interpretable} {Deep} {Learning}.},
	doi = {10.1109/ISBI.2019.8759236},
	abstract = {Current approaches to explaining the decisions of deep learning systems for medical tasks have focused on visualising the elements that have contributed to each decision. We argue that such approaches are not enough to “open the black box” of medical decision making systems because they are missing a key component that has been used as a standard communication tool between doctors for centuries: language. We propose a model-agnostic interpretability method that involves training a simple recurrent neural network model to produce descriptive sentences to clarify the decision of deep learning classifiers. We test our method on the task of detecting hip fractures from frontal pelvic x-rays. This process requires minimal additional labelling despite producing text containing elements that the original deep learning classification model was not specifically trained to detect. The experimental results show that: 1) the sentences produced by our method consistently contain the desired information, 2) the generated sentences are preferred by the cohort of doctors tested compared to current tools that create saliency maps, and 3) the combination of visualisations and generated text is better than either alone.},
	booktitle = {2019 {IEEE} 16th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2019)},
	author = {Gale, William and Oakden-Rayner, Lauren and Carneiro, Gustavo and Palmer, Lyle J. and Bradley, Andrew P.},
	month = apr,
	year = {2019},
	note = {ISSN: 1945-8452},
	keywords = {interpretability, Biomedical imaging, bone, Deep learning, fractures, Hip, Pattern recognition, Task analysis, text generation, Training, x-ray imaging, X-rays},
	pages = {1275--1279}
}


@article{zhang_survey_2021,
	title = {A {Survey} on {Neural} {Network} {Interpretability}},
	url = {http://arxiv.org/abs/2012.14261},
	abstract = {Along with the great success of deep neural networks, there is also growing concern about their black-box nature. The interpretability issue affects people's trust on deep learning systems. It is also related to many ethical problems, e.g., algorithmic discrimination. Moreover, interpretability is a desired property for deep networks to become powerful tools in other research fields, e.g., drug discovery and genomics. In this survey, we conduct a comprehensive review of the neural network interpretability research. We first clarify the definition of interpretability as it has been used in many different contexts. Then we elaborate on the importance of interpretability and propose a novel taxonomy organized along three dimensions: type of engagement (passive vs. active interpretation approaches), the type of explanation, and the focus (from local to global interpretability). This taxonomy provides a meaningful 3D view of distribution of papers from the relevant literature as two of the dimensions are not simply categorical but allow ordinal subcategories. Finally, we summarize the existing interpretability evaluation methods and suggest possible research directions inspired by our new taxonomy.},
	urldate = {2021-05-19},
	journal = {arXiv:2012.14261 [cs]},
	author = {Zhang, Yu and Tiňo, Peter and Leonardis, Aleš and Tang, Ke},
	month = mar,
	year = {2021},
	note = {arXiv: 2012.14261},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, interpretability},
	file = {arXiv Fulltext PDF:/home/csantos/Zotero/storage/U367AETA/Zhang et al. - 2021 - A Survey on Neural Network Interpretability.pdf:application/pdf;arXiv.org Snapshot:/home/csantos/Zotero/storage/L5SSE9WW/2012.html:text/html},
}



@article{pfohl_empirical_2021,
	title = {An {Empirical} {Characterization} of {Fair} {Machine} {Learning} {For} {Clinical} {Risk} {Prediction}},
	volume = {113},
	issn = {15320464},
	url = {http://arxiv.org/abs/2007.10306},
	doi = {10.1016/j.jbi.2020.103621},
	abstract = {The use of machine learning to guide clinical decision making has the potential to worsen existing health disparities. Several recent works frame the problem as that of algorithmic fairness, a framework that has attracted considerable attention and criticism. However, the appropriateness of this framework is unclear due to both ethical as well as technical considerations, the latter of which include trade-offs between measures of fairness and model performance that are not well-understood for predictive models of clinical outcomes. To inform the ongoing debate, we conduct an empirical study to characterize the impact of penalizing group fairness violations on an array of measures of model performance and group fairness. We repeat the analyses across multiple observational healthcare databases, clinical outcomes, and sensitive attributes. We find that procedures that penalize differences between the distributions of predictions across groups induce nearly-universal degradation of multiple performance metrics within groups. On examining the secondary impact of these procedures, we observe heterogeneity of the effect of these procedures on measures of fairness in calibration and ranking across experimental conditions. Beyond the reported trade-offs, we emphasize that analyses of algorithmic fairness in healthcare lack the contextual grounding and causal awareness necessary to reason about the mechanisms that lead to health disparities, as well as about the potential of algorithmic fairness methods to counteract those mechanisms. In light of these limitations, we encourage researchers building predictive models for clinical use to step outside the algorithmic fairness frame and engage critically with the broader sociotechnical context surrounding the use of machine learning in healthcare.},
	urldate = {2021-05-19},
	journal = {Journal of Biomedical Informatics},
	author = {Pfohl, Stephen R. and Foryciarz, Agata and Shah, Nigam H.},
	month = jan,
	year = {2021},
	note = {arXiv: 2007.10306},
	keywords = {Computer Science - Computers and Society, Statistics - Applications, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {103621},
	file = {arXiv Fulltext PDF:/home/csantos/Zotero/storage/VFPU56P2/Pfohl et al. - 2021 - An Empirical Characterization of Fair Machine Lear.pdf:application/pdf;arXiv.org Snapshot:/home/csantos/Zotero/storage/T4GGJM73/2007.html:text/html},
}


@article{rajpurkar_chexnet_2017,
	title = {{CheXNet}: {Radiologist}-{Level} {Pneumonia} {Detection} on {Chest} {X}-{Rays} with {Deep} {Learning}},
	shorttitle = {{CheXNet}},
	url = {http://arxiv.org/abs/1711.05225},
	abstract = {We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer convolutional neural network trained on ChestX-ray14, currently the largest publicly available chest X-ray dataset, containing over 100,000 frontal-view X-ray images with 14 diseases. Four practicing academic radiologists annotate a test set, on which we compare the performance of CheXNet to that of radiologists. We find that CheXNet exceeds average radiologist performance on the F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and achieve state of the art results on all 14 diseases.},
	urldate = {2021-05-21},
	journal = {arXiv:1711.05225 [cs, stat]},
	author = {Rajpurkar, Pranav and Irvin, Jeremy and Zhu, Kaylie and Yang, Brandon and Mehta, Hershel and Duan, Tony and Ding, Daisy and Bagul, Aarti and Langlotz, Curtis and Shpanskaya, Katie and Lungren, Matthew P. and Ng, Andrew Y.},
	month = dec,
	year = {2017},
	note = {arXiv: 1711.05225},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/csantos/Zotero/storage/GGFQGMQB/Rajpurkar et al. - 2017 - CheXNet Radiologist-Level Pneumonia Detection on .pdf:application/pdf;arXiv.org Snapshot:/home/csantos/Zotero/storage/669EG9D8/1711.html:text/html},
}

@article{finlayson_adversarial_2019,
	title = {Adversarial {Attacks} {Against} {Medical} {Deep} {Learning} {Systems}},
	url = {http://arxiv.org/abs/1804.05296},
	abstract = {The discovery of adversarial examples has raised concerns about the practical deployment of deep learning systems. In this paper, we demonstrate that adversarial examples are capable of manipulating deep learning systems across three clinical domains. For each of our representative medical deep learning classifiers, both white and black box attacks were highly successful. Our models are representative of the current state of the art in medical computer vision and, in some cases, directly reflect architectures already seeing deployment in real world clinical settings. In addition to the technical contribution of our paper, we synthesize a large body of knowledge about the healthcare system to argue that medicine may be uniquely susceptible to adversarial attacks, both in terms of monetary incentives and technical vulnerability. To this end, we outline the healthcare economy and the incentives it creates for fraud and provide concrete examples of how and why such attacks could be realistically carried out. We urge practitioners to be aware of current vulnerabilities when deploying deep learning systems in clinical settings, and encourage the machine learning community to further investigate the domain-specific characteristics of medical learning systems.},
	urldate = {2021-05-21},
	journal = {arXiv:1804.05296 [cs, stat]},
	author = {Finlayson, Samuel G. and Chung, Hyung Won and Kohane, Isaac S. and Beam, Andrew L.},
	month = feb,
	year = {2019},
	note = {arXiv: 1804.05296},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/home/csantos/Zotero/storage/88Q57966/Finlayson et al. - 2019 - Adversarial Attacks Against Medical Deep Learning .pdf:application/pdf;arXiv.org Snapshot:/home/csantos/Zotero/storage/MAAHK2S8/1804.html:text/html},
}

@article{rokem_assessment_2017,
	title = {Assessment of the need for separate test set and number of medical images necessary for deep learning: a sub-sampling study},
	copyright = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {Assessment of the need for separate test set and number of medical images necessary for deep learning},
	url = {https://www.biorxiv.org/content/10.1101/196659v2},
	doi = {10.1101/196659},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}Deep learning algorithms have tremendous potential utility in the classification of biomedical images. For example, images acquired with retinal optical coherence tomography (OCT) can be used to accurately classify patients with adult macular degeneration (AMD), and distinguish them from healthy control patients. However, previous research has suggested that large amounts of data are required in order to train deep learning algorithms, because of the large number of parameters that need to be fit. Here, we show that a moderate amount of data (data from approximately 1,800 patients) may be enough to reach close-to-maximal performance in the classification of AMD patients from OCT images. These results suggest that deep learning algorithms can be trained on moderate amounts of data, provided that images are relatively homogenous, and the effective number of parameters is sufficiently small. Furthermore, we demonstrate that in this application, cross-validation with a separate test set that is not used in any part of the training does not differ substantially from cross-validation with a validation data-set used to determine the optimal stopping point for training.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2021-05-21},
	journal = {bioRxiv},
	author = {Rokem, Ariel and Wu, Yue and Lee, Aaron},
	month = oct,
	year = {2017},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {196659},
	file = {Full Text PDF:/home/csantos/Zotero/storage/XV9SE3YJ/Rokem et al. - 2017 - Assessment of the need for separate test set and n.pdf:application/pdf;Snapshot:/home/csantos/Zotero/storage/XQZHH67Q/196659v2.html:text/html},
}




@article{havaei17,
  author    = {Mohammad Havaei and
               Axel Davy and
               David Warde{-}Farley and
               Antoine Biard and
               Aaron C. Courville and
               Yoshua Bengio and
               Chris Pal and
               Pierre{-}Marc Jodoin and
               Hugo Larochelle},
  title     = {Brain tumor segmentation with Deep Neural Networks},
  journal   = {Medical Image Analysis},
  volume    = {35},
  pages     = {18--31},
  year      = {2017},
  url       = {https://doi.org/10.1016/j.media.2016.05.004},
  doi       = {10.1016/j.media.2016.05.004},
  timestamp = {Wed, 17 May 2017 14:25:40 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/mia/HavaeiDWBCBPJL17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{lecun98,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}


@incollection{alexnet2012,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}


@article{raghu2019,
  author    = {Maithra Raghu and
               Chiyuan Zhang and
               Jon M. Kleinberg and
               Samy Bengio},
  title     = {Transfusion: Understanding Transfer Learning with Applications to
               Medical Imaging},
  journal   = {CoRR},
  volume    = {abs/1902.07208},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.07208},
  archivePrefix = {arXiv},
  eprint    = {1902.07208},
  timestamp = {Sat, 02 Mar 2019 16:35:42 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1902-07208},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{abramoff2016,
    author = {Abràmoff, Michael David and Lou, Yiyue and Erginay, Ali and Clarida, Warren and Amelon, Ryan and Folk, James C. and Niemeijer, Meindert},
    title = {Improved Automated Detection of Diabetic Retinopathy on a Publicly Available Dataset Through Integration of Deep Learning},
    journal = {Investigative Ophthalmology \& Visual Science},
    volume = {57},
    number = {13},
    pages = {5200-5206},
    year = {2016},
    month = {10},
    issn = {1552-5783},
    doi = {10.1167/iovs.16-19964},
    url = {https://doi.org/10.1167/iovs.16-19964},
    eprint = {https://arvojournals.org/arvo/content\_public/journal/iovs/935768/i1552-5783-57-13-5200.pdf},
}

@article{gulshan2016,
    author = {Gulshan, Varun and Peng, Lily and Coram, Marc and Stumpe, Martin C. and Wu, Derek and Narayanaswamy, Arunachalam and Venugopalan, Subhashini and Widner, Kasumi and Madams, Tom and Cuadros, Jorge and Kim, Ramasamy and Raman, Rajiv and Nelson, Philip C. and Mega, Jessica L. and Webster, Dale R.},
    title = "{Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs}",
    journal = {JAMA},
    volume = {316},
    number = {22},
    pages = {2402-2410},
    year = {2016},
    month = {12},
    issn = {0098-7484},
    doi = {10.1001/jama.2016.17216},
    url = {https://doi.org/10.1001/jama.2016.17216},
    eprint = {https://jamanetwork.com/journals/jama/articlepdf/2588763/joi160132.pdf},
}

@article{Lancet,
author={Matthew J Burton and Jacqueline Ramke and Ana Patricia Marques and Rupert R A Bourne and Nathan Congdon and Iain Jones and Brandon A M Ah Tong and Simon Arunga and Damodar Bachani and Covadonga Bascaran and Andrew Bastawrous and Karl Blanchet and Tasanee Braithwaite and John C Buchan and John Cairns et al.},
title={The Lancet Global Health Commission on Global Eye Health: vision beyond 2020},
journal={The Lancet Global Health},
year={April 2021},
volume={9},
}

-----




@article{Acharya2015,
  title={Decision support system for the glaucoma using {Gabor} transformation},
  author={Acharya, U Rajendra and Ng, EYK and Eugene, Lim Wei Jie and Noronha, Kevin P and Min, Lim Choo and Nayak, K Prabhakar and Bhandary, Sulatha V},
  journal={Biomedical Signal Processing and Control},
  volume={15},
  pages={18--26},
  year={2015},
  publisher={Elsevier}
}

@article{Issac2015,
  title={An adaptive threshold based image processing technique for improved glaucoma detection and classification},
  author={Issac, Ashish and Sarathi, M Partha and Dutta, Malay Kishore},
  journal={Computer methods and programs in biomedicine},
  volume={122},
  number={2},
  pages={229--244},
  year={2015},
  publisher={Elsevier}
}

@article{Singh2016,
  title={Image processing based automatic diagnosis of glaucoma using wavelet features of segmented optic disc from fundus image},
  author={Singh, Anushikha and Dutta, Malay Kishore and ParthaSarathi, M and Uher, Vaclav and Burget, Radim},
  journal={Computer methods and programs in biomedicine},
  volume={124},
  pages={108--120},
  year={2016},
  publisher={Elsevier}
}

@article{Li2018,
  title={Efficacy of a deep learning system for detecting glaucomatous optic neuropathy based on color fundus photographs},
  author={Li, Zhixi and He, Yifan and Keel, Stuart and Meng, Wei and Chang, Robert T and He, Mingguang},
  journal={Ophthalmology},
  volume={125},
  number={8},
  pages={1199--1206},
  year={2018},
  publisher={Elsevier}
}

@article{Santos2018,
  title={Convolutional neural network and texture descriptor-based automatic detection and diagnosis of glaucoma},
  author={dos Santos Ferreira, Marcos Vinicius and de Carvalho Filho, Antonio Oseas and de Sousa, Alcilene Dalilia and Silva, Aristofanes Correa and Gattass, Marcelo},
  journal={Expert Systems with Applications},
  volume={110},
  pages={250--263},
  year={2018},
  publisher={Elsevier}
}

@ARTICLE{Jiang2020,
  author={Jiang, Yuming and Duan, Lixin and Cheng, Jun and Gu, Zaiwang and Xia, Hu and Fu, Huazhu and Li, Changsheng and Liu, Jiang},
  journal={IEEE Transactions on Biomedical Engineering}, 
  title={{JointRCNN}: A Region-Based Convolutional Neural Network for Optic Disc and Cup Segmentation}, 
  year={2020},
  volume={67},
  number={2},
  pages={335-343},
  doi={10.1109/TBME.2019.2913211}}

@article{Martins2020,
title = {Offline computer-aided diagnosis for {Glaucoma} detection using fundus images targeted at mobile devices},
journal = {Computer Methods and Programs in Biomedicine},
volume = {192},
pages = {105341},
year = {2020},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2020.105341},
url = {https://www.sciencedirect.com/science/article/pii/S0169260719312015},
author = {José Martins and Jaime S. Cardoso and Filipe Soares},
keywords = {Computer-aided diagnosis (CAD), Fundus images, Glaucoma, Deep learning},
abstract = {Background and Objective: Glaucoma, an eye condition that leads to permanent blindness, is typically asymptomatic and therefore difficult to be diagnosed in time. However, if diagnosed in time, Glaucoma can effectively be slowed down by using adequate treatment; hence, an early diagnosis is of utmost importance. Nonetheless, the conventional approaches to diagnose Glaucoma adopt expensive and bulky equipment that requires qualified experts, making it difficult, costly and time-consuming to diagnose large amounts of people. Consequently, new alternatives to diagnose Glaucoma that suppress these issues should be explored. Methods: This work proposes an interpretable computer-aided diagnosis (CAD) pipeline that is capable of diagnosing Glaucoma using fundus images and run offline in mobile devices. Several public datasets of fundus images were merged and used to build Convolutional Neural Networks (CNNs) that perform segmentation and classification tasks. These networks are then used to build a pipeline for Glaucoma assessment that outputs a Glaucoma confidence level and also provides several morphological features and segmentations of relevant structures, resulting in an interpretable Glaucoma diagnosis. To assess the performance of this method in a restricted environment, this pipeline was integrated into a mobile application and time and space complexities were assessed. Results: Considering the test set, the developed pipeline achieved 0.91 and 0.75 of Intersection over Union (IoU) in the optic disc and optic cup segmentation, respectively. With regards to the classification, an accuracy of 0.87 with a sensitivity of 0.85 and an AUC of 0.93 were attained. Moreover, this pipeline runs on an average Android smartphone in under two seconds. Conclusions: The results demonstrate the potential that this method can have in the contribution to an early Glaucoma diagnosis. The proposed approach achieved similar or slightly better metrics than the current CAD systems for Glaucoma assessment while running on more restricted devices. This pipeline can, therefore, be used to construct accurate and affordable CAD systems that could enable large Glaucoma screenings, contributing to an earlier diagnose of this condition.}
}

@article{Mrad2022,
title = {A Fast and Accurate Method for Glaucoma Screening from Smartphone-Captured Fundus Images},
journal = {IRBM},
volume = {43},
number = {4},
pages = {279-289},
year = {2022},
issn = {1959-0318},
doi = {https://doi.org/10.1016/j.irbm.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1959031821000725},
author = {Y. Mrad and Y. Elloumi and M. Akil and M.H. Bedoui},
keywords = {Fundus image, Glaucoma, Feature extraction, Classification, SVM, m-health},
abstract = {The glaucoma is an eye disease that causes blindness when it progresses in an advanced stage. Early glaucoma diagnosis is essential to prevent the vision loss. However, early detection is not covered due to the lack of ophthalmologists and the limited accessibility to retinal image capture devices. In this paper, we present an automated method for glaucoma screening dedicated for Smartphone Captured Fundus Images (SCFIs). The implementation of the method into a smartphone associated to an optical lens for retina capturing leads to a mobile aided screening system for glaucoma. The challenge consists in insuring higher performance detection despite the moderate quality of SCFIs, with a reduced execution time to be adequate for the clinical use. The main idea consists in deducing glaucoma based on the vessel displacement inside the Optic Disk (OD), where the vessel tree remains sufficiently modeled on SCFIs. Within this objective, our major contribution consists in proposing: (1) a robust processing for locating vessel centroids in order to adequately model the vessel distribution, and (2) a feature vector that relevantly reflect two main glaucoma biomarkers in terms of vessel displacement. Furthermore, all processing steps are carefully chosen based on lower complexity, to be suitable for fast clinical screening. A first evaluation of our method is performed using the two public DRISHTI-DB and DRIONS-DB databases, where 99% and 95% accuracy, 96.77% and 97,5% specificity and 100% and 95% sensitivity are respectively achieved. Thereafter, the method is evaluated using two fundus image databases respectively captured through a smartphone and retinograph for the same persons. We achieve 100% accuracy using both databases which assesses the robustness of our method. In addition, the detection is performed on 0.027 and 0.029 second when executed respectively on the Samsung-M51 on the Samsung-A70 smartphone devices. Our proposed smartphone app provides a cost-effective and widely accessible mobile platform for early screening of glaucoma in remote clinics or areas with limited access to fundus cameras and ophthalmologists.}
}

---------


@misc{hooker2021compressed,
      title={What Do Compressed Deep Neural Networks Forget?}, 
      author={Sara Hooker and Aaron Courville and Gregory Clark and Yann Dauphin and Andrea Frome},
      year={2021},
      eprint={1911.05248},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{tang2023scratchticket,
    author    = {Tang, Pengwei and Yao, Wei and Li, Zhicong and Liu, Yong},
    title     = {Fair Scratch Tickets: Finding Fair Sparse Networks Without Weight Training},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {24406-24416}
}

@misc{buscaElsken2019,
      title={Neural Architecture Search: A Survey}, 
      author={Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},
      year={2019},
      eprint={1808.05377},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{podaHoefler2021,
author = {Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
title = {Sparsity in Deep Learning: Pruning and Growth for Efficient Inference and Training in Neural Networks},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, sometimes even better than, the original dense networks. Sparsity promises to reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {241},
numpages = {124},
keywords = {deep learning, low memory, performance, generalization, sparsity}
}



@misc{patricio2022survey,
  doi = {10.48550/ARXIV.2205.04766},
  
  url = {https://arxiv.org/abs/2205.04766},
  
  author = {Patrício, Cristiano and Neves, João C. and Teixeira, Luís F.},
  
  keywords = {Image and Video Processing (eess.IV), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Explainable Deep Learning Methods in Medical Imaging Diagnosis: A Survey},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{mitchell2019cards,
author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
title = {Model Cards for Model Reporting},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287596},
doi = {10.1145/3287560.3287596},
abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {220–229},
numpages = {10},
keywords = {documentation, ML model evaluation, disaggregated evaluation, ethical considerations, model cards, fairness evaluation, datasheets},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}


@article{raghu2018,
  author    = {Maithra Raghu and
               Katy Blumer and
               Rory Sayres and
               Ziad Obermeyer and
               Sendhil Mullainathan and
               Jon M. Kleinberg},
  title     = {Direct Uncertainty Prediction with Applications to Healthcare},
  journal   = {CoRR},
  volume    = {abs/1807.01771},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.01771},
  archivePrefix = {arXiv},
  eprint    = {1807.01771},
  timestamp = {Mon, 13 Aug 2018 16:48:54 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1807-01771},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{wu2019,
author={N. {Wu} and J. {Phang} and J. {Park} and Y. {Shen} and Z. {Huang} and M. {Zorin} and S. {Jastrzebski} and T. {Févry} and J. {Katsnelson} and E. {Kim} and S. {Wolfson} and U. {Parikh} and S. {Gaddam} and L. L. Y. {Lin} and K. {Ho} and J. D. {Weinstein} and B. {Reig} and Y. {Gao} and H. T. K. {Pysarenko} and A. {Lewin} and J. {Lee} and K. {Airola} and E. {Mema} and S. {Chung} and E. {Hwang} and N. {Samreen} and S. G. {Kim} and L. {Heacock} and L. {Moy} and K. {Cho} and K. J. {Geras}},
journal={IEEE Transactions on Medical Imaging},
title={Deep Neural Networks Improve Radiologists’ Performance in Breast Cancer Screening},
year={2019},
volume={},
number={},
pages={1-1},
keywords={Breast cancer;Task analysis;Biomedical imaging;Predictive models;Training;deep learning;deep convolutional neural networks;breast cancer screening;mammography},
doi={10.1109/TMI.2019.2945514},
ISSN={1558-254X},
month={},}



